from __future__ import annotations

import json
import uuid
from typing import AsyncGenerator, Dict, List, Optional

from utils.logger import get_logger
from services.stock_analyzer_service import StockAnalyzerService
from services.stock_data_provider import StockDataProvider
from services.technical_indicator import TechnicalIndicator
from services.stock_scorer import StockScorer
from services.ai_analyzer import AIAnalyzer
import pandas as pd


logger = get_logger()


class AgentOrchestrator:
    """
    Minimal multi-agent orchestrator to support preset-based analysis without
    breaking existing API contracts. For now, presets route to existing
    StockAnalyzerService pipelines, while establishing a unified interface
    for future graph/node orchestration.
    """

    # Built-in presets registry (minimal). In the future this can be loaded
    # from DB tables created by migrations v7-v9.
    BUILTIN_PRESETS: List[Dict] = [
        {
            "id": "standard",
            "name": "æ ‡å‡†ç‰ˆ",
            "description": "æ•°æ®â†’æŒ‡æ ‡â†’è¯„åˆ†â†’LLM æ ‡å‡†æµç¨‹",
            "graph": {
                "nodes": [
                    {"id": "ind", "type": "Indicator"},
                    {"id": "score", "type": "Scoring"},
                    {"id": "llm", "type": "LLMAnalyst"},
                ],
                "edges": [
                    {"from": "ind", "to": "score"},
                    {"from": "score", "to": "llm"},
                ],
            },
            "enabled": True,
            "is_builtin": True,
        },
        {
            "id": "risk_first",
            "name": "é£æ§ä¼˜å…ˆ",
            "description": "åœ¨LLMå‰å¢åŠ é£æ§è¯„ä¼°ï¼ˆå ä½ï¼Œæš‚ä¸æ ‡å‡†ç‰ˆä¸€è‡´ï¼‰",
            "graph": {},
            "enabled": True,
            "is_builtin": True,
        },
        {
            "id": "multi_model_vote",
            "name": "å¤šæ¨¡å‹å…±è¯†",
            "description": "å¤šæ¨¡å‹æŠ•ç¥¨ï¼ˆå ä½ï¼Œæš‚ä¸æ ‡å‡†ç‰ˆä¸€è‡´ï¼‰",
            "graph": {},
            "enabled": True,
            "is_builtin": True,
        },
        {
            "id": "single_model_roles",
            "name": "å•æ¨¡å‹å¤šè§’è‰²",
            "description": "ä¸€ä¸ªæ¨¡å‹ï¼Œé€šè¿‡å¤šè§’è‰²æç¤ºè¯äº§å‡ºå¤šè§†è§’ç»“è®ºï¼Œå¹¶ç”±ç»¼åˆè€…èšåˆä¸ºæ‰§è¡Œå»ºè®®",
            "graph": {},
            "params": {
                "roles": [
                    "trend_analyst",
                    "levels_mapper",
                    "risk_position",
                    "execution_planner",
                    "devils_advocate",
                    "scenario_planner",
                    "synthesizer"
                ]
            },
            "enabled": True,
            "is_builtin": True,
        },
    ]

    def __init__(
        self,
        custom_api_url: Optional[str] = None,
        custom_api_key: Optional[str] = None,
        custom_api_model: Optional[str] = None,
        custom_api_timeout: Optional[str] = None,
    ) -> None:
        self._stock_service = StockAnalyzerService(
            custom_api_url=custom_api_url,
            custom_api_key=custom_api_key,
            custom_api_model=custom_api_model,
            custom_api_timeout=custom_api_timeout,
        )
        # Components for custom pipelines
        self._data_provider = StockDataProvider()
        self._indicator = TechnicalIndicator()
        self._scorer = StockScorer()
        self._api_url = custom_api_url
        self._api_key = custom_api_key
        self._api_model = custom_api_model
        self._api_timeout = custom_api_timeout

    @classmethod
    def list_presets(cls) -> List[Dict]:
        """Return available presets (built-in for now)."""
        return [p for p in cls.BUILTIN_PRESETS if p.get("enabled")]

    @classmethod
    def get_preset(cls, preset_id: Optional[str]) -> Optional[Dict]:
        if not preset_id:
            return None
        for p in cls.BUILTIN_PRESETS:
            if p.get("id") == preset_id and p.get("enabled"):
                return p
        return None

    async def run(
        self,
        stock_codes: List[str],
        market_type: str,
        stream: bool,
        analysis_days: int,
        preset_id: Optional[str] = None,
        portfolio_context: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Execute analysis according to the preset. For MVP, all presets map to
        existing StockAnalyzerService flows to ensure output compatibility.
        
        Args:
            portfolio_context: Optional user portfolio information to include in analysis
        """
        preset = self.get_preset(preset_id)
        preset_key = preset.get("id") if preset else "standard"

        # Generate unique analysis ID
        analysis_id = str(uuid.uuid4())
        
        # Emit a small orchestrator init message (non-breaking, optional fields)
        init_msg = {
            "orchestrator": {
                "preset_id": preset_key,
                "status": "initialized",
                "analysis_id": analysis_id,
            }
        }
        yield json.dumps(init_msg, ensure_ascii=False)

        # Single-model multi-role preset
        if preset_key == "single_model_roles":
            async for chunk in self._run_single_model_roles(stock_codes, market_type, analysis_days, portfolio_context):
                yield chunk
            return

        if len(stock_codes) == 1:
            async for chunk in self._stock_service.analyze_stock(
                stock_codes[0], market_type, stream=stream, analysis_days=analysis_days, portfolio_context=portfolio_context
            ):
                yield chunk
        else:
            async for chunk in self._stock_service.scan_stocks(
                stock_codes,
                market_type=market_type,
                min_score=0,
                stream=stream,
                analysis_days=analysis_days,
            ):
                yield chunk


    async def _run_single_model_roles(
        self,
        stock_codes: List[str],
        market_type: str,
        analysis_days: int,
        portfolio_context: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """Single-model multi-role pipeline with synthesizer aggregation.
        Streams per-role outputs and finishes with a synthesized plan.
        
        Args:
            portfolio_context: Optional user portfolio information to include in analysis
        """
        role_templates = (
            ("æŠ€æœ¯è¶‹åŠ¿åˆ†æå¸ˆ", "ä½ æ˜¯æŠ€æœ¯åˆ†æå¸ˆã€‚åŸºäºæŠ€æœ¯æ‘˜è¦ä¸è¿‘{days}æ—¥æ•°æ®ï¼Œè¾“å‡º: è¶‹åŠ¿(UP/DOWN/FLAT)ã€åŠ¨é‡è´¨é‡ã€å…³é”®è¯æ®(3-5æ¡)ã€å…³é”®æŠ€æœ¯ä½(æ”¯æ’‘/å‹åŠ›)ã€‚æ•°æ®: {summary} è¿‘{days}æ—¥æ•°æ®: {recent} å¸‚åœº:{market} æ ‡çš„:{code}"),
            ("æ”¯æ’‘/å‹åŠ›æ˜ å°„å¸ˆ", "ä½ æ˜¯æ”¯æ’‘/å‹åŠ›æ˜ å°„å¸ˆã€‚ç»™å‡ºæœ€è¿‘æœ‰æ•ˆçš„æ”¯æ’‘/å‹åŠ›ä»·ä½æ•°ç»„(å…·ä½“æ•°å­—)ã€è§¦å‘æ¡ä»¶(çªç ´/è·Œç ´)ã€æ— æ•ˆåŒ–æ¡ä»¶ï¼Œé¿å…å«ç³Šè¡¨è¿°ã€‚æ•°æ®: {recent} å¸‚åœº:{market} æ ‡çš„:{code}"),
            ("æ³¢åŠ¨ä¸ä»“ä½ç®¡ç†å¸ˆ", "ä½ æ˜¯é£é™©ä¸ä»“ä½ç®¡ç†å¸ˆã€‚åŸºäºæ³¢åŠ¨ç‡/é‡èƒ½/RSIå»ºè®®ä»“ä½(0-100%)ã€æ­¢æŸ(ä»·æˆ–æ¯”ä¾‹)ã€æŒä»“å‘¨æœŸ(çŸ­/ä¸­)ï¼Œå¹¶ç»™å‡ºåŠ å‡ä»“è§„åˆ™çš„é‡åŒ–é˜ˆå€¼ã€‚æ‘˜è¦: {summary} æ•°æ®: {recent}"),
            ("äº¤æ˜“æ‰§è¡Œè§„åˆ’å¸ˆ", "ä½ æ˜¯äº¤æ˜“æ‰§è¡Œè§„åˆ’å¸ˆã€‚æŠŠå½“å‰è§‚ç‚¹è½¬ä¸ºæ‰§è¡Œæ¸…å•: å…¥åœºåŒºé—´ã€åˆ†æ‰¹è®¡åˆ’ã€æ­¢æŸã€ç›®æ ‡ä½(1/2/3)ã€å¿…è¦æ‰§è¡Œæ¡ä»¶(å¿…é¡»æ»¡è¶³/æœ€å¥½æ»¡è¶³)ã€‚ç»“åˆå‰è¿°è§’è‰²è¦ç‚¹: {prev}"),
            ("åå¯¹æ„è§å®¡é˜…å®˜", "ä½ æ˜¯åå¯¹æ„è§å®¡é˜…å®˜ã€‚æå‡º3æ¡åå‘é£é™©è®ºæ®åŠå…¶å¤±æ•ˆ/è§¦å‘æ¡ä»¶ï¼Œå¹¶ç»™å‡ºè§¦å‘åçš„åº”å¯¹æ–¹æ¡ˆã€‚ç»“åˆå‰è¿°è¦ç‚¹: {prev}"),
            ("åœºæ™¯æ¨æ¼”è§„åˆ’å¸ˆ", "ä½ æ˜¯åœºæ™¯æ¨æ¼”è§„åˆ’å¸ˆã€‚åœ¨é¡ºåˆ©/éœ‡è¡/åè½¬ä¸‰ç§æƒ…å¢ƒä¸‹ç»™å‡ºä¸åŒçš„è¿›é€€è§„åˆ™ã€æŒä»“/æ­¢ç›ˆè°ƒæ•´ä¸å†æ¬¡éªŒè¯ç‚¹ã€‚ç»“åˆè¦ç‚¹: {prev}"),
        )

        for code in stock_codes:
            # åˆå§‹åŒ–tokenä½¿ç”¨ç»Ÿè®¡
            total_usage = {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
                "estimated_cost": 0.0  # å¦‚æœéœ€è¦çš„è¯
            }
            
            # Init header
            yield json.dumps({"stream_type": "single", "stock_code": code}, ensure_ascii=False)

            # Indicators and basic score (compatible with UI)
            df = await self._data_provider.get_stock_data(code, market_type)
            df_ind = self._indicator.calculate_indicators(df)
            score = self._scorer.calculate_score(df_ind)
            rec0 = self._scorer.get_recommendation(score)
            latest = df_ind.iloc[-1]
            prev = df_ind.iloc[-2] if len(df_ind) > 1 else latest
            price_change_value = float(latest['Close'] - prev['Close'])
            change_percent = latest.get('Change_pct')
            yield json.dumps({
                "stock_code": code,
                "score": int(score),
                "recommendation": rec0,
                "price": float(latest.get('Close', 0)),
                "price_change_value": price_change_value,
                "price_change": change_percent,
                "change_percent": change_percent,
                "rsi": float(latest.get('RSI', 0)) if 'RSI' in latest else None,
                "ma_trend": "UP" if latest.get('MA5', 0) > latest.get('MA20', 0) else "DOWN",
                "macd_signal": "BUY" if latest.get('MACD', 0) > latest.get('MACD_Signal', 0) else "SELL",
                "volume_status": "HIGH" if latest.get('Volume_Ratio', 1) > 1.5 else ("LOW" if latest.get('Volume_Ratio', 1) < 0.5 else "NORMAL"),
                "status": "waiting"
            }, ensure_ascii=False)

            # Build shared summary and recent data
            technical_summary = {
                'trend': 'upward' if df_ind.iloc[-1]['MA5'] > df_ind.iloc[-1]['MA20'] else 'downward',
                'volatility': f"{df_ind.iloc[-1]['Volatility']:.2f}%" if 'Volatility' in df_ind.columns else "",
                'volume_trend': 'increasing' if df_ind.iloc[-1].get('Volume_Ratio', 1) > 1 else 'decreasing',
                'rsi_level': float(df_ind.iloc[-1].get('RSI', 50))
            }
            recent_df = df_ind.tail(analysis_days).copy()
            recent_df.reset_index(inplace=True)
            recent_df.rename(columns={recent_df.columns[0]: 'date'}, inplace=True)
            if pd.api.types.is_datetime64_any_dtype(recent_df['date']):
                recent_df['date'] = recent_df['date'].dt.strftime('%Y-%m-%d')
            else:
                try:
                    recent_df['date'] = pd.to_datetime(recent_df['date']).dt.strftime('%Y-%m-%d')
                except Exception:
                    recent_df['date'] = recent_df['date'].astype(str)
            recent_data = recent_df.to_dict('records')

            # Send chart data chunk
            yield json.dumps({
                "stock_code": code,
                "status": "analyzing",
                "chart_data": recent_data
            }, ensure_ascii=False)

            # Run roles with single model
            ai = AIAnalyzer(
                custom_api_url=self._api_url,
                custom_api_key=self._api_key,
                custom_api_model=self._api_model,
                custom_api_timeout=self._api_timeout,
            )
            collected_text = ""
            for idx, (role_name, tmpl) in enumerate(role_templates, start=1):
                logger.info(f"å¼€å§‹è°ƒç”¨è§’è‰² {role_name} åˆ†æè‚¡ç¥¨ {code} (ç¬¬{idx}/{len(role_templates)}ä¸ªè§’è‰²)")
                
                prompt = tmpl.format(
                    days=analysis_days,
                    summary=technical_summary,
                    recent=recent_data,
                    market=market_type,
                    code=code,
                    prev=collected_text,
                )
                
                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè§’è‰²ä¸”æä¾›äº†portfolio_contextï¼Œæ·»åŠ åˆ°prompt
                if idx == 1 and portfolio_context:
                    logger.info(f"ğŸ“‹ æ£€æµ‹åˆ°æŒä»“ä¿¡æ¯ï¼Œå‡†å¤‡æ·»åŠ åˆ°{role_name}çš„åˆ†ææç¤ºè¯ (é•¿åº¦: {len(portfolio_context)} å­—ç¬¦)")
                    prompt += f"\n\n{'='*50}\nğŸ“Š æˆ‘çš„æŒä»“æƒ…å†µ\n{'='*50}\n{portfolio_context}"
                    logger.info(f"âœ… å·²å°†ç”¨æˆ·æŒä»“ä¿¡æ¯æ·»åŠ åˆ°{role_name}çš„åˆ†ææç¤ºè¯ä¸­")
                    logger.debug(f"å®Œæ•´prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
                elif idx == 1:
                    logger.debug(f"âŒ portfolio_context ä¸ºç©ºï¼Œæœªæ·»åŠ æŒä»“ä¿¡æ¯åˆ°{role_name}")
                
                # å‘é€è§’è‰²å¼€å§‹æ ‡è®°
                yield json.dumps({
                    "stock_code": code,
                    "ai_analysis_chunk": f"<analysis>\n\n### {role_name}\n",
                    "status": "analyzing",
                    "role": role_name,
                    "order": idx
                }, ensure_ascii=False)
                
                role_text = ""
                role_usage = None
                try:
                    # æµå¼æ¥æ”¶è§’è‰²åˆ†æ
                    stream_gen = await ai.get_completion(prompt, stream=True)
                    async for chunk_data in stream_gen:
                        # chunk_data ç°åœ¨æ˜¯ (content, usage) å…ƒç»„
                        if isinstance(chunk_data, tuple):
                            chunk, usage = chunk_data
                            logger.debug(f"æ”¶åˆ°å…ƒç»„: chunké•¿åº¦={len(chunk)}, usage={usage}")
                        else:
                            # å‘åå…¼å®¹ï¼šå¦‚æœä¸æ˜¯å…ƒç»„ï¼Œå½“ä½œçº¯å†…å®¹å¤„ç†
                            chunk = chunk_data
                            usage = None
                            logger.debug(f"æ”¶åˆ°å­—ç¬¦ä¸²: chunké•¿åº¦={len(chunk)}")
                        
                        role_text += chunk
                        
                        # ä¿å­˜usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        if usage:
                            role_usage = usage
                            logger.info(f"è§’è‰² {role_name} æ”¶åˆ°usageä¿¡æ¯: {usage}")
                        
                        # å®æ—¶å‘é€æµå¼ç‰‡æ®µ
                        if chunk:  # åªå‘é€æœ‰å†…å®¹çš„chunk
                            yield json.dumps({
                                "stock_code": code,
                                "ai_analysis_chunk": chunk,
                                "status": "analyzing",
                                "role": role_name,
                                "order": idx
                            }, ensure_ascii=False)
                    
                    # ç´¯ç§¯tokenä½¿ç”¨
                    if role_usage:
                        total_usage["prompt_tokens"] += role_usage.get("prompt_tokens", 0)
                        total_usage["completion_tokens"] += role_usage.get("completion_tokens", 0)
                        total_usage["total_tokens"] += role_usage.get("total_tokens", 0)
                        logger.info(f"ç´¯ç§¯åæ€»tokenä½¿ç”¨: {total_usage}")
                    
                    logger.info(f"è§’è‰² {role_name} åˆ†æå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(role_text)} å­—ç¬¦ï¼Œtokenä½¿ç”¨: {role_usage}")
                except Exception as e:
                    logger.exception(f"Role {role_name} generation failed for stock {code}")
                    # è§’è‰²åˆ†æå¤±è´¥æ—¶ï¼Œå‘é€é”™è¯¯æ¶ˆæ¯å¹¶ç»ˆæ­¢æ•´ä¸ªåˆ†æ
                    error_msg = f"è§’è‰² {role_name} åˆ†æå¤±è´¥: {str(e)}"
                    if "ReadTimeout" in str(type(e).__name__) or "timeout" in str(e).lower():
                        error_msg = f"è§’è‰² {role_name} åˆ†æè¶…æ—¶ï¼ˆå¯èƒ½æ˜¯AIæœåŠ¡å“åº”æ…¢æˆ–è¶…æ—¶è®¾ç½®è¿‡çŸ­ï¼‰"
                    
                    yield json.dumps({
                        "stock_code": code,
                        "status": "error",
                        "error": error_msg
                    }, ensure_ascii=False)
                    return  # ç»ˆæ­¢æ•´ä¸ªåˆ†ææµç¨‹
                
                # å‘é€è§’è‰²ç»“æŸæ ‡è®°
                yield json.dumps({
                    "stock_code": code,
                    "ai_analysis_chunk": "\n</analysis>",
                    "status": "analyzing",
                    "role": role_name,
                    "order": idx
                }, ensure_ascii=False)
                
                collected_text += f"\n\n[{role_name}]\n{role_text}"

            # Synthesizer
            logger.info(f"å¼€å§‹è°ƒç”¨ç»¼åˆå†³ç­–å®˜æ±‡æ€»æ‰€æœ‰è§’è‰²ç»“è®º (è‚¡ç¥¨: {code})")
            synth_prompt = (
                "ä½ æ˜¯ç»¼åˆå†³ç­–å®˜ï¼Œæ±‡æ€»ä¸‹åˆ—å¤šè§’è‰²ç»“è®ºï¼ŒæŒ‰ç»Ÿä¸€ç»“æ„è¾“å‡º: \n"
                "- ç»“è®º(ä¹°å…¥/æŒæœ‰/å–å‡º/è§‚æœ›)\n- è¯æ®(3-5æ¡)\n- è¡ŒåŠ¨(å…¥åœºåŒºé—´/æ­¢æŸ/ç›®æ ‡ä½/ä»“ä½/æ—¶é—´æ¡†)\n- é£é™©(2-3æ¡ä¸è§¦å‘æ¡ä»¶)\n- ç½®ä¿¡åº¦(0-1)\n\n"
                f"å¤šè§’è‰²ç»“è®ºå¦‚ä¸‹:\n{collected_text}\n"
            )
            
            # å‘é€ç»¼åˆå†³ç­–å¼€å§‹æ ‡è®°ï¼ˆä½¿ç”¨ ai_analysis_chunk ä¿æŒä¸€è‡´æ€§ï¼‰
            yield json.dumps({
                "stock_code": code,
                "ai_analysis_chunk": "<final>### ç»¼åˆå†³ç­–å®˜\n\n",
                "status": "analyzing"
            }, ensure_ascii=False)
            
            final_text = ""
            final_usage = None
            try:
                # æµå¼æ¥æ”¶ç»¼åˆå†³ç­–
                stream_gen = await ai.get_completion(synth_prompt, stream=True)
                async for chunk_data in stream_gen:
                    # chunk_data ç°åœ¨æ˜¯ (content, usage) å…ƒç»„
                    if isinstance(chunk_data, tuple):
                        chunk, usage = chunk_data
                    else:
                        # å‘åå…¼å®¹ï¼šå¦‚æœä¸æ˜¯å…ƒç»„ï¼Œå½“ä½œçº¯å†…å®¹å¤„ç†
                        chunk = chunk_data
                        usage = None
                    
                    final_text += chunk
                    
                    # ä¿å­˜usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if usage:
                        final_usage = usage
                        logger.info(f"ç»¼åˆå†³ç­–å®˜æ”¶åˆ°usageä¿¡æ¯: {usage}")
                    
                    # å®æ—¶å‘é€æµå¼ç‰‡æ®µ
                    if chunk:  # åªå‘é€æœ‰å†…å®¹çš„chunk
                        yield json.dumps({
                            "stock_code": code,
                            "ai_analysis_chunk": chunk,
                            "status": "analyzing"
                        }, ensure_ascii=False)
                
                # ç´¯ç§¯ç»¼åˆå†³ç­–çš„tokenä½¿ç”¨
                if final_usage:
                    total_usage["prompt_tokens"] += final_usage.get("prompt_tokens", 0)
                    total_usage["completion_tokens"] += final_usage.get("completion_tokens", 0)
                    total_usage["total_tokens"] += final_usage.get("total_tokens", 0)
                    logger.info(f"ç´¯ç§¯åæ€»tokenä½¿ç”¨: {total_usage}")
                
                logger.info(f"ç»¼åˆå†³ç­–å®˜åˆ†æå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(final_text)} å­—ç¬¦ï¼Œtokenä½¿ç”¨: {final_usage}")
            except Exception as e:
                logger.exception(f"ç»¼åˆå†³ç­–å®˜æ‰§è¡Œå¤±è´¥: {e}")
                # ç»¼åˆå†³ç­–å®˜å¤±è´¥æ—¶ï¼Œå‘é€é”™è¯¯æ¶ˆæ¯å¹¶ç»ˆæ­¢
                error_msg = f"ç»¼åˆå†³ç­–å®˜åˆ†æå¤±è´¥: {str(e)}"
                if "ReadTimeout" in str(type(e).__name__) or "timeout" in str(e).lower():
                    error_msg = f"ç»¼åˆå†³ç­–å®˜åˆ†æè¶…æ—¶ï¼ˆå¯èƒ½æ˜¯AIæœåŠ¡å“åº”æ…¢æˆ–è¶…æ—¶è®¾ç½®è¿‡çŸ­ï¼‰"
                
                yield json.dumps({
                    "stock_code": code,
                    "status": "error",
                    "error": error_msg
                }, ensure_ascii=False)
                return  # ç»ˆæ­¢æ•´ä¸ªåˆ†ææµç¨‹

            # å‘é€ç»¼åˆå†³ç­–ç»“æŸæ ‡è®°
            yield json.dumps({
                "stock_code": code,
                "ai_analysis_chunk": "</final>",
                "status": "analyzing"
            }, ensure_ascii=False)

            # Extract recommendation using existing helper
            rec_text = AIAnalyzer(
                custom_api_url=self._api_url,
                custom_api_key=self._api_key,
                custom_api_model=self._api_model,
                custom_api_timeout=self._api_timeout,
            )._extract_recommendation(final_text)

            # å‘é€å®ŒæˆçŠ¶æ€ï¼ˆåŒ…å«tokenä½¿ç”¨ä¿¡æ¯ï¼‰
            completion_data = {
                "stock_code": code,
                "status": "completed",
                "recommendation": rec_text
            }
            
            # æ·»åŠ tokenä½¿ç”¨ä¿¡æ¯åˆ°å“åº”ä¸­
            logger.info(f"æ£€æŸ¥tokenä½¿ç”¨ç»Ÿè®¡: total_tokens={total_usage['total_tokens']}")
            
            if total_usage["total_tokens"] > 0:
                # æœ‰ç²¾ç¡®çš„tokenç»Ÿè®¡
                completion_data["token_usage"] = total_usage
                logger.info(f"è‚¡ç¥¨ {code} åˆ†æå®Œæˆï¼Œæ€»tokenä½¿ç”¨ï¼ˆç²¾ç¡®ï¼‰: {total_usage}")
            else:
                # APIä¸è¿”å›usageï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼ˆç²—ç•¥ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼‰
                # è®¡ç®—æ‰€æœ‰è§’è‰²çš„è¾“å…¥å­—ç¬¦æ•°ï¼ˆpromptï¼‰
                prompt_chars = 0
                for _, tmpl in role_templates:
                    prompt_chars += len(tmpl) + len(str(technical_summary)) + len(str(recent_data))
                
                # è®¡ç®—ç»¼åˆå†³ç­–çš„è¾“å…¥å­—ç¬¦æ•°
                prompt_chars += len(synth_prompt)
                
                # è®¡ç®—è¾“å‡ºå­—ç¬¦æ•°
                output_chars = len(collected_text) + len(final_text)
                
                # ä¼°ç®—tokenï¼ˆ1 token â‰ˆ 4 å­—ç¬¦ï¼Œä¸­æ–‡å¯èƒ½æ˜¯ 1 token â‰ˆ 1.5-2 å­—ç¬¦ï¼‰
                estimated_tokens = (prompt_chars + output_chars) // 3  # ä½¿ç”¨3ä½œä¸ºä¸­æ–‡å‹å¥½çš„ä¼°ç®—
                
                completion_data["token_usage"] = {
                    "estimated": True,
                    "total_tokens": estimated_tokens,
                    "prompt_chars": prompt_chars,
                    "output_chars": output_chars
                }
                logger.info(f"è‚¡ç¥¨ {code} åˆ†æå®Œæˆï¼Œä¼°ç®—tokenä½¿ç”¨: ~{estimated_tokens} (è¾“å…¥{prompt_chars}å­—ç¬¦ + è¾“å‡º{output_chars}å­—ç¬¦)")
            
            yield json.dumps(completion_data, ensure_ascii=False)


